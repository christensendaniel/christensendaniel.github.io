{
  "id": "2025-09-06-training-llm-part-1-motivation-and-architecture",
  "title": "Training My Own LLM Part 1: Why I Did It and What I Was Getting Into",
  "author": "Daniel Christensen",
  "date": "September 6, 2025",
  "dateISO": "2025-09-06",
  "description": "The motivation behind training a custom large language model from scratch, the architectural decisions that shaped the project, and the honest reality of working within consumer hardware constraints.",
  "tags": [
    "LLM",
    "Machine Learning",
    "GPT-2",
    "Deep Learning",
    "AI"
  ],
  "excerpt": "The motivation behind training a custom large language model from scratch, the architectural decisions that shaped the project, and the honest reality of working within consumer hardware constraints.",
  "content": "<p>There is no shortage of large language models available today. GPT-4, Claude, Llama, Mistral—the list grows every few months. So why spend weeks training one from scratch on a gaming GPU?</p>\n        <p>The honest answer: because I learn better hands-on. Coming from an academic background, I know that reading papers is not the same as writing the proof. I wanted to understand how these systems actually work.</p>\n        <p>This is part one of a five-part series documenting that journey.</p>\n        <h2 id=\"setting-realistic-expectations\">Setting Realistic Expectations</h2>\n        <p>From the outset, I established clear and deliberately modest objectives. The goal was not to compete with state-of-the-art models. It was to build a functional, locally-deployable language model that could serve as both a learning platform and a proof of technical competency.</p>\n        <p>The scope was intentionally constrained:</p>\n        <ul>\n          <li><strong>Context window</strong>: 1024 tokens (versus 100K+ in modern production models)</li>\n          <li><strong>Parameter count</strong>: Millions, not billions</li>\n          <li><strong>Training data</strong>: A curated subset, not internet-scale</li>\n          <li><strong>Compute budget</strong>: One consumer GPU, not a distributed cluster</li>\n        </ul>\n        <p>This was always going to be a toy model. Acknowledging that upfront made every subsequent decision cleaner.</p>\n        <h2 id=\"why-gpt-2\">Why GPT-2?</h2>\n        <p>I researched other models, but the underlying data was readily available for GPT-2, and I could follow the general understanding shared by others online.</p>\n        <p>Larger models like GPT-3 and GPT-4 would require a small datacenter's worth of GPUs. More complex architectures would have doubled memory requirements or demanded multiple machines working in parallel.</p>\n        <p>GPT-2 Small hit the only sweet spot that actually mattered: it was possible to build on my hardware. Its 124 million parameters fit in memory alongside everything needed to train it. It is also battle-tested, with extensive documentation, working examples, and a community that has already debugged the problems I would inevitably face.</p>\n        <h2 id=\"understanding-the-architecture-by-building-it\">Understanding the Architecture by Building It</h2>\n        <h3 id=\"self-attention-finally-made-sense\">Self-Attention Finally Made Sense</h3>\n        <p>The formula <code>Attention(Q,K,V) = softmax(QK^T/√d_k)V</code> had appeared in dozens of papers I had read. It became concrete only when I had to implement it. Watching tensors flow through attention heads during debugging gave me an intuitive grasp of how the model learns to relate different parts of the input sequence.</p>\n        <p>It also helped me understand why a GPU is much faster and preferred for training.</p>\n        <p>GPT-2 Small uses 12 attention heads with 768-dimensional hidden states. During implementation, the quadratic complexity O(n²) with respect to sequence length stopped being an abstract concern and became a practical one. Doubling the context window quadruples memory requirements. On an RTX 4080 with 16GB VRAM, that constraint became real very quickly.</p>\n        <h3 id=\"the-embedding-layer-surprise\">The Embedding Layer Surprise</h3>\n        <p>My first genuine insight came from the embedding layer. Nearly a third of GPT-2's entire capacity is dedicated to understanding individual words and tokens before any actual reasoning happens. The model needs a massive built-in dictionary just to translate human language into something it can work with. This seemed excessive until I understood the implication: if the model cannot richly represent each token, nothing that follows matters.</p>\n        <h3 id=\"positional-encoding-and-the-simultaneity-problem\">Positional Encoding and the Simultaneity Problem</h3>\n        <p>The second breakthrough was positional encoding. GPT does not read a sentence sequentially the way humans do. It sees the entire input at once—every token simultaneously. This creates an immediate problem: without position information, \"The cat chased the dog\" and \"The dog chased the cat\" would appear identical.</p>\n        <p>The solution is elegant: the model learns what \"being the first token\" or \"being the hundredth token\" actually means through training. It is not hardcoded. The model figures out on its own that early tokens often set context, and that nearby tokens tend to relate more closely. This is completely invisible when chatting with a production model, but it is foundational to why these systems work at all.</p>\n        <h2 id=\"working-within-hardware-limits\">Working Within Hardware Limits</h2>\n        <p>The RTX 4080 with 16GB VRAM was the limiting factor in every architectural decision. Here is the concrete reality of training on consumer hardware:</p>\n        <ul>\n          <li><strong>Model weights</strong>: 124M parameters × 4 bytes (FP32) ≈ 500MB</li>\n          <li><strong>Optimizer states</strong>: Adam requires 2× model size for momentum and variance ≈ 1GB</li>\n          <li><strong>Activations and gradients</strong>: With batch size 4, I was constantly monitoring memory usage</li>\n        </ul>\n        <p>I implemented gradient checkpointing not as a strategic optimization but because I had to. Without it, I could only fit a batch size of 1, which made training unstable. (And I needed to be able to pause the training when it was time to use my gaming computer for what I purchased it for.) The technique recomputes activations during backpropagation instead of storing them.</p>\n        <h3 id=\"what-the-scaling-laws-papers-do-not-emphasize\">What the Scaling Laws Papers Do Not Emphasize</h3>\n        <p>The most humbling realization: my RTX 4080 achieved approximately 45 tokens per second during inference—roughly 1/100th the speed of commercial APIs. That single comparison drove home the infrastructure gap between hobbyist experiments and production systems.</p>\n        <h2 id=\"what-comes-next\">What Comes Next</h2>\n        <p>Part 2 covers dataset engineering: why OpenWebText was the right choice, what tokenization looks like at scale, and what it feels like to watch all 28 CPU cores hit 100% utilization for the first time.</p>\n        <p><em>This is part 1 of a 5-part series.</em></p>\n        <ul>\n          <li><em>Part 1: Motivation and Architecture (this post)</em></li>\n          <li><em><a href=\"/blog/2025-09-13-training-llm-part-2-dataset-engineering\">Part 2: Dataset Engineering</a></em></li>\n          <li><em><a href=\"/blog/2025-09-20-training-llm-part-3-implementation-and-training-loop\">Part 3: Implementation and the Training Loop</a></em></li>\n          <li><em><a href=\"/blog/2025-09-27-training-llm-part-4-the-training-experience\">Part 4: The Training Experience</a></em></li>\n          <li><em><a href=\"/blog/2025-10-04-training-llm-part-5-evaluation-deployment-conclusions\">Part 5: Evaluation, Deployment, and Conclusions</a></em></li>\n        </ul>"
}