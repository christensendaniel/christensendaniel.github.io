{
  "id": "2025-09-20-training-llm-part-3-implementation-and-training-loop",
  "title": "Training My Own LLM Part 3: Implementation and the Training Loop",
  "author": "Daniel Christensen",
  "date": "September 20, 2025",
  "dateISO": "2025-09-20",
  "description": "The actual model configuration, memory arithmetic that dictated every decision, gradient accumulation, and the checkpoint strategy earned through repeated failures.",
  "tags": [
    "LLM",
    "PyTorch",
    "Hugging Face",
    "Training",
    "GPU"
  ],
  "excerpt": "The actual model configuration, memory arithmetic that dictated every decision, gradient accumulation, and the checkpoint strategy earned through repeated failures.",
  "content": "<p>The model configuration was essentially copied from GPT-2 Small's published specifications. No clever innovations, no architectural improvements — just 124 million parameters arranged exactly as OpenAI designed them in 2019.</p>\n        <p>When learning how transformers work, the last thing you want is to debug your own modifications.</p>\n        <h2 id=\"configuration\">Configuration</h2>\n        <pre><code>from transformers import GPT2Config, GPT2LMHeadModel\ndef create_model():\n    config = GPT2Config(\n        vocab_size=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        use_cache=False\n    )\n    model = GPT2LMHeadModel(config)\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total parameters: {total_params:,}\")       # 124,439,808\n    print(f\"Model size: {total_params * 4 / 1024**2:.0f} MB\")  # ~474 MB\n    return model</code></pre>\n        <p>These numbers are not arbitrary. The hidden dimension of 768 divides evenly by the 12 attention heads, giving each head 64 dimensions — a size optimized for GPU tensor cores that operate efficiently on multiples of 64. This is the kind of implementation detail that only becomes obvious when you have to think about it explicitly, and when I'm avoiding OOM errors.</p>\n        <h2 id=\"the-memory-arithmetic\">The Memory Arithmetic</h2>\n        <p>The batch size of 4 seems small. Here is why it was not a choice:</p>\n        <pre><code>def calculate_memory(batch_size=4, seq_length=1024,\n                     hidden_size=768, num_layers=12):\n    model_params = 124_439_808 * 4          # FP32 bytes\n    attention_mem = batch_size * seq_length * hidden_size * 5 * 4\n    ffn_mem = batch_size * seq_length * hidden_size * 4 * 4\n    per_layer = attention_mem + ffn_mem\n    total_activations = per_layer * num_layers\n    optimizer_mem = model_params * 2        # Adam momentum + variance\n    total_gb = (model_params + total_activations + optimizer_mem) / (1024**3)\n    print(f\"Batch size {batch_size}: ~{total_gb:.1f} GB\")\ncalculate_memory(4)   # ~11 GB - fits\ncalculate_memory(8)   # ~19 GB - out of memory</code></pre>\n        <p>Batch size 4 fit. Batch size 8 did not. That was the entire decision. Gradient accumulation over 8 steps gave an effective batch size of 32 while processing only 4 examples at a time:</p>\n        <pre><code>model.zero_grad()\naccumulated_loss = 0\nfor micro_step in range(gradient_accumulation_steps):\n    outputs = model(input_ids=batch[micro_step])\n    loss = outputs.loss / gradient_accumulation_steps\n    loss.backward()\n    accumulated_loss += loss.item()\noptimizer.step()\nscheduler.step()</code></pre>\n        <p>Computationally inefficient. Memory efficient. The correct tradeoff given the constraints.</p>\n        <h2 id=\"training-arguments-that-actually-worked\">Training Arguments That Actually Worked</h2>\n        <p>The training configuration below is the result of multiple failed runs, not first-attempt intuition:</p>\n        <pre><code>from transformers import TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-clone\",\n    overwrite_output_dir=False,   # Never overwrite after losing progress\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=1000,            # Critical for stability\n    fp16=True,\n    optim=\"adamw_torch\",\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    gradient_checkpointing=True,\n    logging_steps=100,\n    save_steps=500,\n    save_total_limit=5,\n    save_safetensors=True,\n    evaluation_strategy=\"steps\",\n    eval_steps=1000,\n    resume_from_checkpoint=\"latest\",\n)</code></pre>\n        <p>The warmup schedule deserves emphasis. Without it, the model diverged within the first 100 steps of every attempt. The cosine decay that follows keeps the learning rate from oscillating during the long tail of training.</p>\n        <h2 id=\"checkpoint-recovery\">Checkpoint Recovery</h2>\n        <p>Losing a weekend of training to a power outage teaches you to treat checkpointing as infrastructure rather than convenience, and plus I like using my gaming computer for what I purchased it for.</p>\n        <p>As a 100% side note I started training from a checkpoint, went to Alaska hoping it would finish while I was gone, then the power went out about 12 hours after I left so I lost about 460 hours of possible training which was. . . Sad. But back to my actual writing</p>\n        <pre><code>from pathlib import Path\ndef find_latest_checkpoint(output_dir=\"./gpt2-clone\"):\n    checkpoints = list(Path(output_dir).glob(\"checkpoint-*\"))\n    if not checkpoints:\n        return None\n    checkpoints.sort(key=lambda x: int(x.name.split(\"-\")[1]))\n    latest = checkpoints[-1]\n    required_files = [\"config.json\", \"model.safetensors\", \"trainer_state.json\"]\n    for f in required_files:\n        if not (latest / f).exists():\n            print(f\"Checkpoint corrupted, missing {f}\")\n            return None\n    return str(latest)</code></pre>\n        <p>The training run spanned five epochs across approximately 640,000 steps. At 500-step checkpoints, keeping the last five meant roughly 7GB of checkpoint storage at any given time. A small price.</p>\n        <h2 id=\"what-the-numbers-looked-like\">What the Numbers Looked Like</h2>\n        <p>Training for five epochs meant:</p>\n        <ul>\n          <li>Total training steps: ~640,000</li>\n          <li>Time per epoch: ~14 hours</li>\n          <li>Total training time: ~100 hours, excluding failed runs</li>\n          <li>Checkpoints saved: ~1,280 total across the run</li>\n        </ul>\n        <p>The most educational moments were not the successful steps — they were the failures. Each crash taught something concrete about memory scaling, gradient stability, or checkpoint integrity.</p>\n        <p>By the math it was about 70 hours, but the practical run-time was more from check pointing, and power-cycles. I don't have a clear record of how long it took.</p>\n        <h2 id=\"what-comes-next\">What Comes Next</h2>\n        <p>Part 4 covers the actual training experience: what the loss curves looked like, the failures that shaped the final configuration, and what 70 hours of watching a GPU run at full load actually teaches you.</p>\n        <p><em>This is part 3 of a 5-part series.</em></p>\n        <ul>\n          <li><em><a href=\"/blog/2025-09-06-training-llm-part-1-motivation-and-architecture\">Part 1: Motivation and Architecture</a></em></li>\n          <li><em><a href=\"/blog/2025-09-13-training-llm-part-2-dataset-engineering\">Part 2: Dataset Engineering</a></em></li>\n          <li><em>Part 3: Implementation and the Training Loop (this post)</em></li>\n          <li><em><a href=\"/blog/2025-09-27-training-llm-part-4-the-training-experience\">Part 4: The Training Experience</a></em></li>\n          <li><em><a href=\"/blog/2025-10-04-training-llm-part-5-evaluation-deployment-conclusions\">Part 5: Evaluation, Deployment, and Conclusions</a></em></li>\n        </ul>"
}