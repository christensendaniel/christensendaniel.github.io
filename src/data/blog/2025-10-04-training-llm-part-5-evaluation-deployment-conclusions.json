{
  "id": "2025-10-04-training-llm-part-5-evaluation-deployment-conclusions",
  "title": "Training My Own LLM Part 5: Evaluation, Deployment, and Honest Conclusions",
  "author": "Daniel Christensen",
  "date": "October 4, 2025",
  "dateISO": "2025-10-04",
  "description": "How the model actually performed, what a local deployment looked like, why I stopped before fine-tuning, and the honest verdict on whether training your own LLM from scratch is worth it.",
  "tags": [
    "LLM",
    "Machine Learning",
    "Deployment",
    "AI",
    "Lessons Learned"
  ],
  "excerpt": "How the model actually performed, what a local deployment looked like, why I stopped before fine-tuning, and the honest verdict on whether training your own LLM from scratch is worth it.",
  "content": "<p>After many days and an increased energy bill, the model was trained. The question was whether it was worth anything — and whether the project as a whole was worth it.</p>\n        <p>The answers are different for each question.</p>\n        <h2 id=\"evaluation-keeping-expectations-calibrated\">Evaluation: Keeping Expectations Calibrated</h2>\n        <h3 id=\"what-the-numbers-said\">What the Numbers Said</h3>\n        <ul>\n          <li><strong>Training loss</strong>: Converged to ~2.8</li>\n          <li><strong>Validation perplexity</strong>: ~30, closely matching published GPT-2 Small benchmarks</li>\n          <li><strong>Inference speed</strong>: 45 tokens per second</li>\n        </ul>\n        <p>Matching the published perplexity for GPT-2 Small was genuinely satisfying. It confirmed the implementation was correct and the training had proceeded as intended.</p>\n        <p>The inference speed was more sobering. 45 tokens per second on an RTX 4080 is approximately 1/100th the throughput of commercial APIs. A conversational exchange that feels instantaneous through an API becomes a patience exercise locally.</p>\n        <h3 id=\"what-the-benchmarks-would-have-said\">What the Benchmarks Would Have Said</h3>\n        <p>I did not run formal benchmarks — MMLU, HellaSwag, or similar evaluations. The model is too small for the results to be meaningful, and the comparison to any current production model would have been discouraging rather than informative. The evaluation that mattered was whether the implementation matched the reference architecture. It did.</p>\n        <h3 id=\"what-the-text-generation-actually-looked-like\">What the Text Generation Actually Looked Like</h3>\n        <p>The model generates coherent text, mostly. Short sequences are reasonable. Longer generations drift. There is a clear quality gap compared to even GPT-2 Medium — which has twice the parameters and was trained on more data. This was expected and confirmed the scaling law predictions.</p>\n        <p>It is not a useful tool. It is a working demonstration of the mechanism.</p>\n        <h2 id=\"deployment-local-only\">Deployment: Local Only</h2>\n        <h3 id=\"what-the-setup-looked-like\">What the Setup Looked Like</h3>\n        <ul>\n          <li>Model quantized to INT8 for a 2× inference speedup with minor quality loss</li>\n          <li>Simple Flask API for local testing</li>\n          <li>Final model size: 550MB</li>\n          <li>Startup time: 3 to 4 seconds on consumer hardware</li>\n        </ul>\n        <pre><code>from transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom flask import Flask, request, jsonify\nimport torch\napp = Flask(__name__)\nmodel = GPT2LMHeadModel.from_pretrained(\"./gpt2-clone/final\")\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nmodel.eval()\n@app.route(\"/generate\", methods=[\"POST\"])\ndef generate():\n    prompt = request.json.get(\"prompt\", \"\")\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=100,\n            temperature=0.8,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return jsonify({\"generated\": text})</code></pre>\n        <h3 id=\"why-there-was-no-production-deployment\">Why There Was No Production Deployment</h3>\n        <p>Cloud hosting would cost more per month than using the OpenAI API directly. A model this size with this quality does not justify the infrastructure. The local deployment was a playground for experimentation, not a practical tool.</p>\n        <h2 id=\"why-i-stopped-before-fine-tuning\">Why I Stopped Before Fine-Tuning</h2>\n        <p>Fine-tuning was the natural next step — adapting the base model for specific domains or tasks using LoRA or QLoRA. I chose not to continue for straightforward reasons.</p>\n        <p>One, I had already done fine-tuning before I made my own model locally. Second, the base model is barely functional for general text generation compared to current standards. Fine-tuning a weak base model produces a weak specialized model. Each fine-tuned variant would require quality domain-specific training data, another 20 to 40 hours of training, and evaluation time I did not want to spend on a model I already knew was obsolete.</p>\n        <p>The diminishing returns were clear. The learning objective had already been met.</p>\n        <h2 id=\"the-honest-verdict\">The Honest Verdict</h2>\n        <h3 id=\"what-this-project-was-worth\">What This Project Was Worth</h3>\n        <p>The educational value was real and cannot be replicated by reading. I now have a working intuition for concepts that were previously abstract: why context windows have the sizes they do, what memory bandwidth actually constrains during training, why warmup schedules exist, and what scaling laws mean in practice.</p>\n        <p>When I read about new architectures, I can contextualize them against direct experience rather than theory. That foundation is genuinely useful for the work I actually do — fine-tuning existing models, building RAG pipelines, integrating LLMs into production systems.</p>\n        <h3 id=\"what-this-project-was-not-worth\">What This Project Was Not Worth</h3>\n        <p>I spent roughly $150 in electricity and 55 days of intermittent effort to replicate 2019 technology at a fraction of its original quality. While I was doing this, the field moved through GPT-3, ChatGPT, GPT-4, and a generation of open-source models that make my result irrelevant from any practical standpoint.</p>\n        <h3 id=\"would-i-do-it-again\">Would I Do It Again?</h3>\n        <p>No. Not for practical purposes.</p>\n        <p>This exercise is the equivalent of computing a regression by hand in a graduate statistics course — foundational for building intuition, worthwhile exactly once, and something you would never do again when the goal is actually getting an answer.</p>\n        <p>If I were teaching a course on LLMs, I would absolutely recommend this as a learning exercise. The hands-on intuition it builds is irreplaceable. But for anyone whose goal is building useful systems with language models, the time is better spent learning to fine-tune existing checkpoints, design effective RAG architectures, and build reliable evaluation frameworks.</p>\n        <p>Use the infrastructure that exists. Understand it deeply. Build on top of it rather than beside it.</p>\n        <h2 id=\"the-full-series\">The Full Series</h2>\n        <ul>\n          <li><em><a href=\"/blog/2025-09-06-training-llm-part-1-motivation-and-architecture\">Part 1: Motivation and Architecture</a></em></li>\n          <li><em><a href=\"/blog/2025-09-13-training-llm-part-2-dataset-engineering\">Part 2: Dataset Engineering</a></em></li>\n          <li><em><a href=\"/blog/2025-09-20-training-llm-part-3-implementation-and-training-loop\">Part 3: Implementation and the Training Loop</a></em></li>\n          <li><em><a href=\"/blog/2025-09-27-training-llm-part-4-the-training-experience\">Part 4: The Training Experience</a></em></li>\n          <li><em>Part 5: Evaluation, Deployment, and Conclusions (this post)</em></li>\n        </ul>\n        <p>Have questions or want to discuss any part of this project? The code is available at <a href=\"https://github.com/christensendaniel\" target=\"_blank\" rel=\"noopener noreferrer\">github.com/christensendaniel</a>.</p>"
}