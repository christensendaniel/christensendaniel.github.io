{
  "id": "2025-09-13-training-llm-part-2-dataset-engineering",
  "title": "Training My Own LLM Part 2: Dataset Engineering at Scale",
  "author": "Daniel Christensen",
  "date": "September 13, 2025",
  "dateISO": "2025-09-13",
  "description": "How I prepared 8.2 million training examples from OpenWebText, what tokenization looks like at scale, and the data pipeline lessons learned the hard way.",
  "tags": [
    "LLM",
    "Machine Learning",
    "NLP",
    "Dataset",
    "Python"
  ],
  "excerpt": "How I prepared 8.2 million training examples from OpenWebText, what tokenization looks like at scale, and the data pipeline lessons learned the hard way.",
  "content": "<p>Dataset selection was one of the easiest decisions in this entire project.</p>\n        <h2 id=\"why-openwebtext\">Why OpenWebText</h2>\n        <p>OpenWebText is an open-source recreation of GPT-2's original training dataset, containing approximately 40GB of filtered web content. Rather than spending weeks implementing web scrapers, parsing HTML, and building quality filters from scratch, I could focus on the model itself.</p>\n        <pre><code>from datasets import load_dataset\ndataset = load_dataset(\"openwebtext\", trust_remote_code=True)\nprint(f\"Dataset size: {len(dataset['train'])} documents\")\n# Output: Dataset size: 8,013,769 documents</code></pre>\n        <p>Conveniently using OpenWebText someone else already did the hard work by removing duplicates, filtering low-quality pages, and ensuring diverse content across domains. News articles, blog posts, forum discussions, creative writing.</p>\n        <h2 id=\"tokenization-reality\">Tokenization Reality</h2>\n        <p>Using GPT-2's tokenizer was a practical necessity, but it also revealed how much the field has progressed as i read about other current transformers across their size of tokens, but it also illustrated a lot more of the how-the-model-works.</p>\n        <pre><code>from transformers import GPT2TokenizerFast\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nprint(f\"Vocabulary size: {tokenizer.vocab_size}\")  # 50,257\ntext = \"The quick brown fox jumps over the lazy dog\"\ntokens = tokenizer.encode(text)\nprint(f\"{len(text)} characters -> {len(tokens)} tokens\")\n# Output: 44 characters -> 10 tokens</code></pre>\n        <p>GPT-2's vocabulary of 50,257 tokens seems large until you compare it to modern models using 100,000+. Fewer tokens means more tokens are needed to represent the same text. My 1024-token context window might represent 3,000 to 4,000 characters â€” while modern models with 128K contexts can handle entire books.</p>\n        <p>The inefficiency becomes more visible with non-English content, and emojis:</p>\n        <pre><code>samples = [\n    \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n    \"ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ\",   # Hello world in Japanese\n    \"ðŸš€ Machine Learning is awesome! ðŸŽ‰\"\n]\nfor text in samples:\n    tokens = tokenizer.encode(text)\n    print(f\"{text[:40]} -> {len(tokens)} tokens\")</code></pre>\n        <p>Code tokenizes relatively efficiently. Japanese and emoji content explodes into many tokens. This is the kind of practical constraint that only becomes visible when you actually run the numbers.</p>\n        <h2 id=\"building-the-data-pipeline\">Building the Data Pipeline</h2>\n        <p>Processing 8 million documents required a pipeline that could use available hardware effectively. My machine has 28 CPU cores and 94GB of RAM â€” normally occupied by Chrome tabs. The tokenization job finally justified both. :)</p>\n        <pre><code>from datasets import load_dataset\nfrom transformers import GPT2TokenizerFast\ndef prepare_dataset(save_dir=\"./tokenized_data\"):\n    dataset = load_dataset(\"openwebtext\", trust_remote_code=True)\n    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], return_attention_mask=False)\n    tokenized = dataset.map(\n        tokenize_function,\n        batched=True,\n        num_proc=28,\n        remove_columns=[\"text\"],\n        desc=\"Tokenizing dataset\"\n    )\n    block_size = 1024\n    def group_texts(examples):\n        concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = (len(concatenated[\"input_ids\"]) // block_size) * block_size\n        result = {\n            k: [t[i:i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in concatenated.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n    lm_dataset = tokenized.map(\n        group_texts,\n        batched=True,\n        num_proc=28,\n        desc=\"Chunking into blocks\"\n    )\n    lm_dataset.save_to_disk(save_dir)\n    print(f\"Final dataset: {len(lm_dataset['train'])} training examples\")\n    return lm_dataset</code></pre>\n        <p>Watching all 28 cores hit 100% utilization was satisfying in a way that is difficult to explain. (My gaming PC gets bored with such a nice CPU) Parallel processing reduced tokenization time considerably.</p>\n        <h2 id=\"the-chunking-strategy\">The Chunking Strategy</h2>\n        <p>Each training example needed to be exactly 1024 tokens for efficient batching. This meant some text got split mid-sentence, which is actually beneficial â€” it forces the model to learn representations that do not depend on always seeing complete sentences.</p>\n        <p>The final dataset: 8.2 million training examples, each containing exactly 1024 tokens. That is 8.4 billion tokens total â€” a meaningful corpus for training, though still tiny compared to GPT-3's 300 billion.</p>\n        <p>The key point I learned with tokenization was all of the odd things that come with it. How does all of human writing get broken-down, what are the inefficiencies of having some tokens exist but not be used, and it spawned a lot of reading about how other models tokenize differently, such as Meta's Llama.</p>\n        <h2 id=\"what-comes-next\">What Comes Next</h2>\n        <p>Part 3 covers the model implementation: configuration decisions, the training loop, memory arithmetic, and the checkpoint strategy that evolved from painful experience.</p>\n        <p><em>This is part 2 of a 5-part series.</em></p>\n        <ul>\n          <li><em><a href=\"/blog/2025-09-06-training-llm-part-1-motivation-and-architecture\">Part 1: Motivation and Architecture</a></em></li>\n          <li><em>Part 2: Dataset Engineering (this post)</em></li>\n          <li><em><a href=\"/blog/2025-09-20-training-llm-part-3-implementation-and-training-loop\">Part 3: Implementation and the Training Loop</a></em></li>\n          <li><em><a href=\"/blog/2025-09-27-training-llm-part-4-the-training-experience\">Part 4: The Training Experience</a></em></li>\n          <li><em><a href=\"/blog/2025-10-04-training-llm-part-5-evaluation-deployment-conclusions\">Part 5: Evaluation, Deployment, and Conclusions</a></em></li>\n        </ul>"
}