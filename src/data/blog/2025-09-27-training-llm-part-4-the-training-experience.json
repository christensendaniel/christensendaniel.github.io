{
  "id": "2025-09-27-training-llm-part-4-the-training-experience",
  "title": "Training My Own LLM Part 4: The Actual Training Experience",
  "author": "Daniel Christensen",
  "date": "September 27, 2025",
  "dateISO": "2025-09-27",
  "description": "What 70 hours of GPU training actually looks like â€” the failures, the recoveries, the thermal throttling, and what the loss curves revealed about the gap between toy models and production systems.",
  "tags": [
    "LLM",
    "Machine Learning",
    "GPU",
    "Training",
    "AI"
  ],
  "excerpt": "What 70 hours of GPU training actually looks like â€” the failures, the recoveries, the thermal throttling, and what the loss curves revealed about the gap between toy models and production systems.",
  "content": "<p>The papers make training look clean. Loss goes down. Model improves. You evaluate, publish, move on.</p>\n        <p>The reality involved power outages, corrupted checkpoints, and at least one complete restart from scratch because I changed the path and forgot to update the code ðŸ˜…</p>\n        <p>This is what the actual experience looked like.</p>\n        <h2 id=\"the-hardware-setup\">The Hardware Setup</h2>\n        <ul>\n          <li>Single RTX 4080, 16GB VRAM</li>\n          <li>No distributed training, no multi-GPU setup</li>\n          <li>Mixed precision FP16 â€” essential, not optional</li>\n          <li>Actual GPU utilization: ~78%, memory bound rather than compute bound</li>\n          <li>Windows â€” because it was what I had</li>\n        </ul>\n        <p>CUDA setup on Windows consumed two days that are not coming back. Dependency conflicts between PyTorch, CUDA 11.8, and various supporting libraries required more troubleshooting than any single step in the project. This is not documented well anywhere, and the error messages are not helpful.</p>\n        <p>If I were to do this again I'd use UV and be better with my requirements so then the dependencies could be addressed easier, and there's a lot of Nvidia packages I needed to install alongside</p>\n        <h2 id=\"what-worked\">What Worked</h2>\n        <p><strong>Resuming from checkpoints.</strong> The checkpoint recovery logic described in Part 3 saved the project at least three times. A power outage mid-training, a pause for my own gaming computer enjoyment, and an accidental path update I missed that resulting in a complete restarts.</p>\n        <p><strong>Mixed precision training.</strong> FP16 delivered a 1.7Ã— speedup and allowed larger effective batch sizes. This was not a nice-to-have optimization â€” it was the difference between a manageable training run and one that would have taken twice as long.</p>\n        <p><strong>Pre-tokenizing the entire dataset.</strong> The first training attempt loaded and tokenized data on the fly. Despite using PyTorch's DataLoader with 4 workers, the GPU was frequently idle waiting for data. Pre-tokenizing and saving the full 45GB processed dataset to disk eliminated this bottleneck entirely.</p>\n        <h2 id=\"what-did-not-work\">What Did Not Work</h2>\n        <p><strong>Batch size 8.</strong> Out of memory on the first step. Expected in hindsight, frustrating in the moment.</p>\n        <p><strong>Training without learning rate warmup.</strong> The first two attempts diverged within 100 steps. Loss climbed instead of falling. Adding 1,000 warmup steps with a cosine decay schedule fixed this completely.</p>\n        <h2 id=\"the-loss-curves\">The Loss Curves</h2>\n        <p>Training loss converged to approximately 2.8 over five epochs. Validation perplexity settled around 30 â€” closely matching published GPT-2 Small results, which was genuinely validating after all the failed attempts.</p>\n        <p>The convergence pattern revealed something the papers understate: smaller models are more sensitive to hyperparameter choices. The loss curves were noisier, the convergence less smooth, and the margin between a good configuration and a diverging one was narrower than I expected. Larger models tolerate more variation. This one did not.</p>\n        <h2 id=\"the-real-cost\">The Real Cost</h2>\n        <p>The project ran intermittently over 55 days. Actual GPU runtime was approximately 300ish hours â€” two weeks of continuous training spread across evenings and weekends.</p>\n        <p>My electricity bill increased by roughly $150 during the peak months, though some of that increase reflected summer cooling costs rather than pure training overhead. Running a consumer GPU at sustained maximum load also ages the hardware faster than normal use.</p>\n        <p>None of this was prohibitive. But the economics put things in perspective: I spent meaningful time and money to train a model worse than GPT-2, which was released in 2019, which is now ancient by the standards of the field.</p>\n        <h2 id=\"what-70-hours-teaches-you\">What 70 Hours Teaches You</h2>\n        <p>The understanding earned from understanding how the models are built, at least at an elementary level was helpful. Watching loss curves plateau didn't particularly matter, it's not like I am an original designer but there was still debugging. I now have an intuitive grasp of concepts that were previously just equations in papers, and hype that people talk about.</p>\n        <p>When I read about new architectures or training techniques, I can immediately place them against my own experience. That context is genuinely valuable and would not exist without the hands-on work.</p>\n        <p>But I would not do it again for practical purposes. The gap between hobbyist experiments and production systems is not just large â€” it is economically insurmountable on consumer hardware. This experience taught me to deeply respect the infrastructure behind modern AI development, while also clarifying where my time is better spent.</p>\n        <h2 id=\"what-comes-next\">What Comes Next</h2>\n        <p>Part 5 covers evaluation, the local deployment that resulted, fine-tuning considerations, and the final honest assessment of what this project was worth.</p>\n        <p><em>This is part 4 of a 5-part series.</em></p>\n        <ul>\n          <li><em><a href=\"/blog/2025-09-06-training-llm-part-1-motivation-and-architecture\">Part 1: Motivation and Architecture</a></em></li>\n          <li><em><a href=\"/blog/2025-09-13-training-llm-part-2-dataset-engineering\">Part 2: Dataset Engineering</a></em></li>\n          <li><em><a href=\"/blog/2025-09-20-training-llm-part-3-implementation-and-training-loop\">Part 3: Implementation and the Training Loop</a></em></li>\n          <li><em>Part 4: The Training Experience (this post)</em></li>\n          <li><em><a href=\"/blog/2025-10-04-training-llm-part-5-evaluation-deployment-conclusions\">Part 5: Evaluation, Deployment, and Conclusions</a></em></li>\n        </ul>"
}